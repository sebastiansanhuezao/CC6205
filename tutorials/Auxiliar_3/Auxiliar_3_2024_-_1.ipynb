{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWM5IugWi5ZV"
      },
      "source": [
        "# Auxiliar 3\n",
        "\n",
        "\n",
        "## üìö Objetivos de la clase üìö\n",
        "\n",
        "La clase auxiliar de esta semana tendr√° varios objetivos:\n",
        "\n",
        "- Explicar el problema de la representaci√≥n Bag of Words.\n",
        "- Motivaci√≥n y repaso de qu√© son los Word Embeddings.\n",
        "- Explicaci√≥n de Word2Vec.\n",
        "- Entrenar nuestros propios `word embeddings`.\n",
        "- Utilizaremos estos embeddings pre-entrenados para mejorar la capacidad de nuestros modelos en tareas nuevas y en la resuelta en el auxiliar pasado.\n",
        "\n",
        "Una vez resuelto, pueden utilizar cualquier parte del c√≥digo que les parezca prudente para la Tarea 3 üòä."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e-9vzuizLe"
      },
      "source": [
        "## **Motivaci√≥n**\n",
        "\n",
        "Partamos por decir que una red neuronal no es m√°s que una serie de operaciones matem√°ticas sobre vectores con una gran cantidad de dimensiones (tensores). Por ende, si queremos entrenar un modelo necesitamos transformar el texto original a vectores num√©ricos.\n",
        "\n",
        "Una de las soluciones m√°s simples a este problema es la representaci√≥n de Bag of Words (BoW). Si aplicamos este m√©todo a cada palabra de cada documento, tendremos un vector one hot encoding por cada palabra. Esto quiere decir que tendremos vectores del largo del vocabulario $V$, con un 1 en la posici√≥n asociada a la palabra representada.\n",
        "\n",
        "Y estamos listos? Podemos entrenar redes neuronales?\n",
        "\n",
        "La verdad es que no es as√≠, y es que estamos ignorando un gran problema con este enfoque. üòû\n",
        "\n",
        "\n",
        "### El gran problema de Bag of Words\n",
        "\n",
        "Pensemos en estas 3 frases como documentos:\n",
        "\n",
        "- $doc_1$: `¬°Buen√≠sima la marraqueta!`\n",
        "- $doc_2$: `¬°Estuvo espectacular ese pan franc√©s!`\n",
        "- $doc_3$: `!Buen√≠sima esa pintura!`\n",
        "\n",
        "Sabemos $doc_1$ y $doc_2$ hablan de lo mismo üçûüçûüëå y que $doc_3$ üé® no tiene mucho que ver con los otros.\n",
        "\n",
        "Supongamos que queremos ver que tan similares son ambos documentos.\n",
        "Para esto, generamos un modelo `Bag of Words` sobre el documento, aplicando este m√©todo por cada palabra para luego tener la representaci√≥n final.\n",
        "\n",
        "\n",
        "Es decir, transformamos cada palabra a un vector one-hot y luego los sumamos por documento.\n",
        "\n",
        "Por simplicidad, omitiremos algunas stopwords y consideramos pan frances como un solo token. As√≠ nos quedar√≠a el siguiente vocabulario:\n",
        "\n",
        "$$v = \\{buen√≠sima, marraqueta, estuvo, espectacular, pan\\ franc√©s, pintura\\}$$\n",
        "\n",
        "Entonces, el $\\vec{doc_1}$ quedar√°:\n",
        "\n",
        "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} +\n",
        "  \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} =\n",
        "  \\begin{bmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix}$$\n",
        "\n",
        "El $\\vec{doc_2}$ quedar√°:\n",
        "\n",
        "$$\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} +\n",
        "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\\\ 0\\end{bmatrix} +\n",
        "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 0\\end{bmatrix} =\n",
        "  \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\\\ 0\\end{bmatrix}$$\n",
        "\n",
        "Y el $\\vec{doc_3}$:\n",
        "\n",
        "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} +\n",
        "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix} =\n",
        "  \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "\n",
        "**¬øCu√°l es el problema?**\n",
        "\n",
        "`buen√≠sima` $\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan ideas muy similares. Por otra parte, sabemos que `marraqueta` $\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `pan franc√©s` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\0\\end{bmatrix}$ se refieren al mismo objeto. Pero en este modelo, estos **son totalmente distintos**. Es decir, los vectores de las palabras que `buen√≠sima` y `espectacular` son tan distintas como `marraqueta` y `pan franc√©s`. Esto se debe a que cada palabra ocupa una dimensi√≥n distinta a las dem√°s y son completamente independientes. Esto evidentemente, repercute en la calidad de los modelos que creamos a partir de nuestro Bag of Words.\n",
        "\n",
        "![BoW](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/BoW-Problem.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlJ6GZe0eUNH"
      },
      "source": [
        "Ahora, si queremos ver que documento es mas similar a otro usando distancia euclidiana, veremos que:\n",
        "\n",
        "$$d(doc_1, doc_2) = 2.236$$\n",
        "$$d(doc_1, doc_3) = 1.414$$\n",
        "\n",
        "Es decir, $doc_1$ se parece mas a $doc_3$ aunque nosotros sabemos que $doc_1$ y $doc_2$ nos est√°n diciendo lo mismo!\n",
        "\n",
        "\n",
        "Nos gustar√≠a que eso no sucediera. Que existiera alg√∫n m√©todo que nos permitiera hacer que palabras similares tengan representaciones similares. Y que con estas, representemos mejor a los documentos, sin asumir que en el espacio son geom√©tricamente equidistantes, ya que esto no es verdad en la vida real.\n",
        "\n",
        "\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjpL3pfplg0B"
      },
      "source": [
        "## **Hip√≥tesis Distribucional**\n",
        "\n",
        "Estamos buscando alg√∫n enfoque que nos permita representar las palabras de forma no aislada, de manera tal que adem√°s capture el significado de esta.\n",
        "\n",
        "Pensemos un poco en la **hip√≥tesis distribucional**. Esta plantea que:\n",
        "\n",
        "    \"Palabras que ocurren en contextos iguales tienden a tener significados similares.\"\n",
        "\n",
        "O equivalentemente,\n",
        "\n",
        "    \"Una palabra es caracterizada por la compa√±√≠a que esta lleva.\"\n",
        "\n",
        "Esto nos puede hacer pensar que podr√≠amos usar los contextos de las palabras para generar vectores que describan mejor dichas palabras: en otras palabras, los `Distributional Vectors`.\n",
        "\n",
        "Por ejemplo, complete la siguiente frase:\n",
        "\n",
        "Pintar√© la muralla de mi casa de color _____\n",
        "\n",
        "Puede ser rojo, blanco, mostaza, etc..\n",
        "\n",
        "Son palabras que a uno se les viene a la mente s√≥lo mirando el contexto entregado, por ende podr√≠amos decir que esas son palabras similares, o al menos muy distintas a Murci√©lago.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ3Y_C-6ltRn"
      },
      "source": [
        "### Opci√≥n 1: Word-Context Matrix\n",
        "\n",
        "Es una matriz donde cada celda $(i,j)$ representa la co-ocurrencia entre una palabra objetivo/centro $w_i$ y un contexto $c_j$. El contexto son las palabras dentro de ventana de tama√±o $k$ que rodean la palabra central.\n",
        "\n",
        "Cada fila representa a una palabra a trav√©s de su contexto. Como pueden ver, ya no es un vector one-hot, si no que ahora contiene mayor informaci√≥n.\n",
        "\n",
        "El tama√±o de la matriz es el tama√±o del vocabulario $V$ al cuadrado. Es decir $|V|*|V|$.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dccuchile/CC6205/master/slides/pics/distributionalSocher.png\" alt=\"Word-context matrices\" style=\"width: 400px;\"/>\n",
        "\n",
        "\n",
        "**Problema: Creada a partir de un corpus respetable, es gigantezca**.\n",
        "\n",
        "Por ejemplo, para $|v| = 100.000$, la matriz tendr√° $\\frac{100000 * 100000 * 4}{10^9} = 40gb $. (Recordando que un entero ocupara 4 bytes)\n",
        "\n",
        "- Es caro mantenerla en memoria.\n",
        "- Los clasificadores no funcionan tan bien con tantas dimensiones (ver [maldici√≥n de la dimensionalidad](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)).\n",
        "\n",
        "**¬øHabr√° una mejor soluci√≥n?**\n",
        "\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxB0APe3S5b2"
      },
      "source": [
        "### Opci√≥n 2: **Word Embeddings**\n",
        "\n",
        "Es una de las representaciones m√°s populares del vocabulario de un corpus. La idea principal de los Word Embeddings es crear representaciones vectoriales densas y de baja dimensionalidad $(d << |V|)$ de las palabras a partir de su contexto.\n",
        "\n",
        "Volvamos a nuestro ejemplo anterior: `buen√≠sima` y `espectacular` ocurren muchas veces en el mismo contexto, por lo que los embeddings que los representan debiesen ser muy similares... (*ejemplos de mentira hechos a mano*):\n",
        "\n",
        "`buen√≠sima` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `marraqueta`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cu√°l es claramente distinto.\n",
        "\n",
        "\n",
        "Pero, ¬øCu√°l es la utilidad de de crear estos vectores en NLP o en el √°rea de Machine Learning en general?\n",
        "\n",
        "Supongamos que tienen una enfermedad grave y deben ser operados el d√≠a de ma√±ana. Le dan a elegir entre ser operados por un estudiante de primer a√±o de medicina con algo de conocimiento m√©dico o bien ser operados por un ni√±o de 5 a√±os üë∂. ¬øA qui√©n elegir√≠as?\n",
        "\n",
        "Espero que tu opci√≥n haya sido el estudiante con una peque√±a noci√≥n de los t√©rminos m√©dicos implicados en una intervenci√≥n as√≠. Algo as√≠ es lo que se quiso lograr en el [paper](https://arxiv.org/abs/1301.3781) presentado por Mikolov en 2013, aludiendo a la herramienta **Word2Vec**. La idea es que si quieres resolver por ejemplo una tarea de clasificaci√≥n de texto, ¬øno ser√≠a √∫til utilizar el conocimiento de alg√∫n modelo pre-entrenado en una tarea similar de texto?. Claro, ser√≠a √∫til partir con los pesos entrenados por otra red, realizando lo que se llama **transfer learning**.\n",
        "\n",
        "Ya pero.. ¬øC√≥mo generamos estos vectores? ¬øC√≥mo podemos capturar el contexto? ¬øCu√°l ser√≠a esa task auxiliar a utilizar?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBGzM6DvGWTk"
      },
      "source": [
        "##### **Word2vec y Skip-gram**\n",
        "\n",
        "Word2Vec es probablemente el paquete de software mas famoso para crear word embeddings utilizando distintos modelos que emplean redes neuronales *shallow* o poco profundas.\n",
        "\n",
        "Este nos provee herramientas para crear distintos tipos de modelos, tales como `Skip-Gram` y `Continuous Bag of Word (CBOW)`. En este caso, solo veremos `Skip-Gram`.\n",
        "\n",
        "**Skip-gram** es una task auxiliar con la que crearemos nuestros embeddings. Esta tarea involucra tanto a las palabras y al contexto de ellas. Consiste en que por cada palabra del dataset, debemos predecir las palabras de su contexto (las palabras presentes en ventana de alg√∫n tama√±o $k$).\n",
        "\n",
        "![Overview](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/overview-skipgram.png)\n",
        "\n",
        "Para resolverla, usaremos una red de una sola capa oculta. Los pesos ya entrenados de esta capa ser√°n los que usaremos como embeddings.\n",
        "\n",
        "#### Detalles del Modelo\n",
        "\n",
        "- Como dijimos, el modelo ser√° una red de una sola capa. La capa oculta tendr√° una dimensi√≥n $d$ la cual nosotros determinaremos. Esta capa no tendr√° funci√≥n de activaci√≥n. Sin embargo, la de salida si, la cual ser√° una softmax para obtener las distribuciones de probabilidades y as√≠ ver cu√°les palabras pertenecen o no al contexto.\n",
        "\n",
        "- El vector de entrada, de tama√±o $|V|$, ser√° un vector one-hot de la palabra que estemos viendo en ese momento.\n",
        "\n",
        "- La salida, tambi√©n de tama√±o $|V|$, ser√° un vector que contenga la distribuci√≥n de probabilidad de que cada palabra del vocabulario pertenezca al contexto de la palabra de entrada.\n",
        "\n",
        "- Al entrenar, se comparar√° la distribuci√≥n de los contextos con la suma de los vectores one-hot del contexto real.\n",
        "\n",
        "\n",
        "(marraqueta, Estuvo), (marraqueta, buenisima), (marraqueta, la)\n",
        "![Skip Gram](https://raw.githubusercontent.com/dccuchile/CC6205/master/tutorials/recursos/Skip-gram.png)\n",
        "\n",
        "\n",
        "Nota: Esto es computacionalmente una locura. Por cada palabra de entrada, debemos calcular la probabilidad de aparici√≥n de todas las otras. Imaginen el caso de un vocabulario de 100.000 de palabras y de 10000000 oraciones...\n",
        "\n",
        "La soluci√≥n a esto es modificar la task a *Negative Sampling*. Esta transforma este problema de $|V|$ clases a uno binario. Sin embargo, no lo veremos por el tiempo, pero est√°n muy bien explicado en el [video de la c√°tedra](https://www.youtube.com/watch?v=XDxzQ7JU95U&feature=youtu.be).\n",
        "\n",
        "\n",
        "### La capa Oculta y los Embeddings\n",
        "\n",
        "Al terminar el entrenamiento, ¬øQu√© nos queda en la capa oculta?\n",
        "\n",
        "Una matriz de $v$ filas por $d$ columnas, la cual contiene lo que buscabamos: Una representaci√≥n continua de todas las palabras de nuestro vocabulario.  \n",
        "\n",
        "**Cada fila de la matriz es un vector que contiene la representaci√≥n continua una palabra del vocabulario.**\n",
        "\n",
        "\n",
        "<img src=\"http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png\" alt=\"Capa Oculta 1\" style=\"width: 400px;\"/>\n",
        "\n",
        "¬øC√≥mo la usamos eficientemente?\n",
        "\n",
        "Simple: usamos los mismos vectores one-hot de la entrada y las multiplicamos por la matriz:\n",
        "\n",
        "<img src=\"http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png\" alt=\"Skip Gram\" style=\"width: 400px;\"/>\n",
        "\n",
        "### Visualizaci√≥n\n",
        "\n",
        "Veamos c√≥mo se ven los embeddings de Word2Vec entrenados sobre un corpus gigante en Ingl√©s. Para facilitar el an√°lisis se reducen las 200 dimensiones a 3. El link a la visualizaci√≥n es el siguiente: Visualizaci√≥n: https://projector.tensorflow.org/\n",
        "\n",
        "### Espacio multidimensional\n",
        "\n",
        "Teniendo nuestro embeddings entonces podr√≠amos hacer operaciones tan interesantes como las siguientes:\n",
        "\n",
        "Manzana + P√∫rpura -> Ciruela\n",
        "\n",
        "Rey - Hombre + Mujer -> Reina\n",
        "\n",
        "Si bien no es posible obtener exactamente dichos vectores, esperar√≠amos que las palabras m√°s cercanas al vector resultante ser√≠an las entregadas, obteniendo as√≠ un significado de las palabras seg√∫n su contexto.\n",
        "\n",
        "\n",
        "### Fuentes\n",
        "\n",
        "Word2vec:\n",
        "- mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
        "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
        "\n",
        "Gensim:\n",
        "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
        "\n",
        "Nota: Las √∫ltimas 2 imagenes pertenecen a [Chris McCormick](http://mccormickml.com/about/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFjukOXkzrYt"
      },
      "source": [
        "# Contextualized word embeddings: BERT, ELMO, FLAIR.\n",
        "\n",
        "x = 'El banco estaba lleno.'\n",
        "y = 'El banco de sangre necesita personal.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7cUyx_-SQHi"
      },
      "source": [
        "## **Entrenar nuestros Embeddings**\n",
        "\n",
        "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementaci√≥n de `word2vec`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-17T20:26:56.028758Z",
          "start_time": "2020-05-17T20:26:52.011925Z"
        },
        "id": "WsA-mAO-SQHi"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import multiprocessing\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# visualizaciones\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from ipywidgets import widgets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcbzFpYSSQHj"
      },
      "source": [
        "### Cargar el dataset y limpiar\n",
        "\n",
        "Nota: Pandas descomprime por si mismo el archivo bz2. Pueden descomprimirlo manualmente usando 7zip."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDOyxxnz2WKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90Vu65q62Wcn",
        "outputId": "46bf8c96-d95e-4d70-f0db-049757394ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-17T20:27:38.022904Z",
          "start_time": "2020-05-17T20:26:56.037686Z"
        },
        "id": "1gsKlG2dSQHj"
      },
      "source": [
        "# descargamos el dataset completo (~40mb)\n",
        "dataset = pd.read_json(\n",
        "    'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_clean.bz2',\n",
        "    encoding=\"utf-8\")\n",
        "\n",
        "dataset_r = dataset.copy(deep = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "cUi8ovST0d7r",
        "outputId": "eb173a9e-ffba-4d5f-9060-770429eb251a"
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               author                  author_link  \\\n",
              "0           Yerko Roa          /lista/autores/yroa   \n",
              "1  Valentina Gonz√°lez     /lista/autores/vgonzalez   \n",
              "2      Felipe Delgado      /lista/autores/fdelgado   \n",
              "3         Mat√≠as Vega         /lista/autores/mvega   \n",
              "4  Valentina Gonz√°lez     /lista/autores/vgonzalez   \n",
              "5   Gonzalo Cifuentes    /lista/autores/gcifuentes   \n",
              "6       Catalina D√≠az  /lista/autores/catalinadiaz   \n",
              "7         Mat√≠as Vega         /lista/autores/mvega   \n",
              "8         Emilio Lara         /lista/autores/elara   \n",
              "9  Valentina Gonz√°lez     /lista/autores/vgonzalez   \n",
              "\n",
              "                                               title  \\\n",
              "0  Colapsa otro segmento de casa que se derrumb√≥ ...   \n",
              "1  Polic√≠a busca a mujer acusada de matar a su pa...   \n",
              "2  Dos detenidos en Liceo de Aplicaci√≥n: protagon...   \n",
              "3  Apoyo transversal: Senado aprueba en general p...   \n",
              "4  Evacuaci√≥n espont√°nea en Instituto Nacional po...   \n",
              "5  Alcalde Sharp lamenta mortal derrumbe y afirma...   \n",
              "6  Joven resulta grave tras ser apu√±alado en even...   \n",
              "7  En prisi√≥n preventiva queda acusado de violar ...   \n",
              "8  Cualquier chileno puede ser objeto de escuchas...   \n",
              "9  Nuevos p√≥rticos y reducci√≥n de espera en sem√°f...   \n",
              "\n",
              "                                                link  category  \\\n",
              "0  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
              "1  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
              "2  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
              "3  https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
              "4  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
              "5  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
              "6  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
              "7  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
              "8  https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
              "9  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
              "\n",
              "            subcategory                                            content  \\\n",
              "0  region-de-valparaiso    Noticia en Desarrollo  Estamos recopilando m...   \n",
              "1  region-metropolitana    Detectives de la Polic√≠a de Investigaciones ...   \n",
              "2  region-metropolitana    Dos detenidos fue el saldo de una serie de i...   \n",
              "3                 chile    La sala del Senado aprob√≥ en general el proy...   \n",
              "4  region-metropolitana    La ma√±ana de este mi√©rcoles se produjo una e...   \n",
              "5  region-de-valparaiso    El alcalde de Valpara√≠so, Jorge Sharp , se r...   \n",
              "6   region-de-los-lagos    Un joven se encuentra en estado grave, tras ...   \n",
              "7      region-del-maule    Un caso criminal -que era investigado de man...   \n",
              "8                 chile    Bajo car√°cter privado, como dictamina la Ley...   \n",
              "9  region-metropolitana    El alcalde Joaqu√≠n Lav√≠n anunci√≥ medidas lue...   \n",
              "\n",
              "                                                tags  \\\n",
              "0                                                 []   \n",
              "1  [#parricidio, #PDI, #Pudahuel, #Regi√≥n Metropo...   \n",
              "2  [#Incendio, #Liceo de Aplicaci√≥n, #Regi√≥n Metr...   \n",
              "3  [#Inmigraci√≥n, #Inmigrantes, #Ley, #Migraci√≥n,...   \n",
              "4  [#Carabineros, #FFEE, #Gases Lacrim√≥genos, #In...   \n",
              "5  [#Alcalde, #derrumbe, #derrumbe en valpara√≠so,...   \n",
              "6  [#Agresi√≥n, #Apu√±alado, #Carabineros, #Chilo√©,...   \n",
              "7  [#Deficiencia mental, #Regi√≥n del Maule, #Talc...   \n",
              "8  [#Ej√©rcito, #escuchas telef√≥nicas, #Espionaje,...   \n",
              "9  [#asaltos a conductores, #encerronas, #las Con...   \n",
              "\n",
              "                                      embedded_links  publication_datetime  \n",
              "0                                                 []         1565778000000  \n",
              "1  [https://media.biobiochile.cl/wp-content/uploa...         1565771820000  \n",
              "2                                                 []         1565772480000  \n",
              "3  [https://media.biobiochile.cl/wp-content/uploa...         1565772720000  \n",
              "4                                                 []         1565772960000  \n",
              "5                                                 []         1565773080000  \n",
              "6  [https://media.biobiochile.cl/wp-content/uploa...         1565773140000  \n",
              "7                                                 []         1565774520000  \n",
              "8  [https://media.biobiochile.cl/wp-content/uploa...         1565774820000  \n",
              "9                                                 []         1565776560000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f55ccfad-e1d4-4ba5-b1f4-72654008975a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>author_link</th>\n",
              "      <th>title</th>\n",
              "      <th>link</th>\n",
              "      <th>category</th>\n",
              "      <th>subcategory</th>\n",
              "      <th>content</th>\n",
              "      <th>tags</th>\n",
              "      <th>embedded_links</th>\n",
              "      <th>publication_datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Yerko Roa</td>\n",
              "      <td>/lista/autores/yroa</td>\n",
              "      <td>Colapsa otro segmento de casa que se derrumb√≥ ...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>region-de-valparaiso</td>\n",
              "      <td>Noticia en Desarrollo  Estamos recopilando m...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1565778000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Valentina Gonz√°lez</td>\n",
              "      <td>/lista/autores/vgonzalez</td>\n",
              "      <td>Polic√≠a busca a mujer acusada de matar a su pa...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>region-metropolitana</td>\n",
              "      <td>Detectives de la Polic√≠a de Investigaciones ...</td>\n",
              "      <td>[#parricidio, #PDI, #Pudahuel, #Regi√≥n Metropo...</td>\n",
              "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
              "      <td>1565771820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Felipe Delgado</td>\n",
              "      <td>/lista/autores/fdelgado</td>\n",
              "      <td>Dos detenidos en Liceo de Aplicaci√≥n: protagon...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>region-metropolitana</td>\n",
              "      <td>Dos detenidos fue el saldo de una serie de i...</td>\n",
              "      <td>[#Incendio, #Liceo de Aplicaci√≥n, #Regi√≥n Metr...</td>\n",
              "      <td>[]</td>\n",
              "      <td>1565772480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mat√≠as Vega</td>\n",
              "      <td>/lista/autores/mvega</td>\n",
              "      <td>Apoyo transversal: Senado aprueba en general p...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>chile</td>\n",
              "      <td>La sala del Senado aprob√≥ en general el proy...</td>\n",
              "      <td>[#Inmigraci√≥n, #Inmigrantes, #Ley, #Migraci√≥n,...</td>\n",
              "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
              "      <td>1565772720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Valentina Gonz√°lez</td>\n",
              "      <td>/lista/autores/vgonzalez</td>\n",
              "      <td>Evacuaci√≥n espont√°nea en Instituto Nacional po...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>region-metropolitana</td>\n",
              "      <td>La ma√±ana de este mi√©rcoles se produjo una e...</td>\n",
              "      <td>[#Carabineros, #FFEE, #Gases Lacrim√≥genos, #In...</td>\n",
              "      <td>[]</td>\n",
              "      <td>1565772960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Gonzalo Cifuentes</td>\n",
              "      <td>/lista/autores/gcifuentes</td>\n",
              "      <td>Alcalde Sharp lamenta mortal derrumbe y afirma...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>region-de-valparaiso</td>\n",
              "      <td>El alcalde de Valpara√≠so, Jorge Sharp , se r...</td>\n",
              "      <td>[#Alcalde, #derrumbe, #derrumbe en valpara√≠so,...</td>\n",
              "      <td>[]</td>\n",
              "      <td>1565773080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Catalina D√≠az</td>\n",
              "      <td>/lista/autores/catalinadiaz</td>\n",
              "      <td>Joven resulta grave tras ser apu√±alado en even...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>region-de-los-lagos</td>\n",
              "      <td>Un joven se encuentra en estado grave, tras ...</td>\n",
              "      <td>[#Agresi√≥n, #Apu√±alado, #Carabineros, #Chilo√©,...</td>\n",
              "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
              "      <td>1565773140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mat√≠as Vega</td>\n",
              "      <td>/lista/autores/mvega</td>\n",
              "      <td>En prisi√≥n preventiva queda acusado de violar ...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>region-del-maule</td>\n",
              "      <td>Un caso criminal -que era investigado de man...</td>\n",
              "      <td>[#Deficiencia mental, #Regi√≥n del Maule, #Talc...</td>\n",
              "      <td>[]</td>\n",
              "      <td>1565774520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Emilio Lara</td>\n",
              "      <td>/lista/autores/elara</td>\n",
              "      <td>Cualquier chileno puede ser objeto de escuchas...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>chile</td>\n",
              "      <td>Bajo car√°cter privado, como dictamina la Ley...</td>\n",
              "      <td>[#Ej√©rcito, #escuchas telef√≥nicas, #Espionaje,...</td>\n",
              "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
              "      <td>1565774820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Valentina Gonz√°lez</td>\n",
              "      <td>/lista/autores/vgonzalez</td>\n",
              "      <td>Nuevos p√≥rticos y reducci√≥n de espera en sem√°f...</td>\n",
              "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
              "      <td>nacional</td>\n",
              "      <td>region-metropolitana</td>\n",
              "      <td>El alcalde Joaqu√≠n Lav√≠n anunci√≥ medidas lue...</td>\n",
              "      <td>[#asaltos a conductores, #encerronas, #las Con...</td>\n",
              "      <td>[]</td>\n",
              "      <td>1565776560000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f55ccfad-e1d4-4ba5-b1f4-72654008975a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f55ccfad-e1d4-4ba5-b1f4-72654008975a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f55ccfad-e1d4-4ba5-b1f4-72654008975a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-17T20:27:38.136649Z",
          "start_time": "2020-05-17T20:27:38.050830Z"
        },
        "id": "xEBQb14tSQHj"
      },
      "source": [
        "# unir titulo con contenido de la noticia\n",
        "content = dataset['title'] + dataset['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkG6dS9V476U",
        "outputId": "bfb5fbc2-5979-4601-cb75-5e5297389c01"
      },
      "source": [
        "content.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Colapsa otro segmento de casa que se derrumb√≥ ...\n",
              "1    Polic√≠a busca a mujer acusada de matar a su pa...\n",
              "2    Dos detenidos en Liceo de Aplicaci√≥n: protagon...\n",
              "3    Apoyo transversal: Senado aprueba en general p...\n",
              "4    Evacuaci√≥n espont√°nea en Instituto Nacional po...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "HWNd77cWUrd2",
        "outputId": "ece2d87c-7376-4c77-fd05-35168557d1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-17T20:27:38.183476Z",
          "start_time": "2020-05-17T20:27:38.175498Z"
        },
        "id": "8upF3fu-SQHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b6f78e-1dc4-4486-f078-c2cc918a66da"
      },
      "source": [
        "content.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26413,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-17T20:27:38.227359Z",
          "start_time": "2020-05-17T20:27:38.221374Z"
        },
        "id": "Y28NxiNySQHk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e59dee43-8b66-42f2-f172-54eadd07f98c"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T21:21:12.916380Z",
          "start_time": "2020-05-07T21:21:04.087799Z"
        },
        "id": "1uvRtm_USQHk"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# limpiar puntuaciones y separar por tokens.\n",
        "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "\n",
        "def simple_tokenizer(doc, lower=False):\n",
        "    if lower:\n",
        "        tokenized_doc = doc.translate(str.maketrans(\n",
        "            '', '', punctuation)).lower().split()\n",
        "\n",
        "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
        "    tokenized_doc = [\n",
        "        token for token in tokenized_doc if token.lower() not in stopwords\n",
        "    ]\n",
        "    return tokenized_doc\n",
        "\n",
        "cleaned_content = [simple_tokenizer(doc) for doc in content.values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T21:21:15.069544Z",
          "start_time": "2020-05-07T21:21:15.063560Z"
        },
        "id": "KqH5QEmzSQHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9947c8c-7d02-47b7-beb7-3a4f2aa68c4b"
      },
      "source": [
        "print(\"Ejemplo de alguna noticia: {}\".format(cleaned_content[14]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo de alguna noticia: ['Ministra', 'Cubillos', 'extensi√≥n', 'paro', 'docente', 'problema', 'propuesta', 'ministra', 'Educaci√≥n', 'Marcela', 'Cubillos', 'refiri√≥', 'a', 'votaci√≥n', 'realizada', 'interior', 'Colegio', 'Profesores', 'decidi√≥', 'mantener', 'paro', 'pese', 'llamado', 'presidente', 'gremio', 'Mario', 'Aguilar', 'a', 'replegarse', 'entrevista', 'programa', 'Expreso', 'B√≠o', 'B√≠o', 'Radio', 'ministra', 'valor√≥', 'dichos', 'Aguilar', 'previo', 'a', 'votaci√≥n', 'asegurando', 'efecto', 'positivo', 'vuelta', 'clases', 'Paro', 'profesores', 'extiende', 'semana', '255', 'votos', 'marcaron', 'diferencia', 'sufragio', '95', 'colegios', 'funcionan', 'normalidad', 'bajaron', '490', 'colegios', 'quedaban', 'paro', 'a', 'Ministerio', 'seguir√°', 'negociando', 'ofrecer√°', 'considerando', 'mayor√≠a', 'vot√≥', 'extender', 'movilizaci√≥n', 'ministra', 'limit√≥', 'a', 'esperar√°n', 'resuelva', 'di√°logos', 'internos', 'llevando', 'a', 'cabo', 'interior', 'gremio', 'l√≠nea', 'sostuvo', 'decisi√≥n', 'presidente', 'colegio', 'punto', 'negociaci√≥n', 'llamando', 'a', 'aceptar', 'propuesta', 'problema', 'propuesta', 'esperamos', 'opiniones', 'interior', 'colegio', 'decanten', 'Magisterio', 'pide', 'a', 'docentes', 'cesar', 'paro', 'oferta', '45', 'mil', 'trimestrales', 'a', 'educadoras', 'diferenciales', 'Consultada', 'd√≥nde', 'problema', 'a', 'parecer', 'jefa', 'cartera', 'reiter√≥', 'di√°logo', 'interno', 'pendiente', 'reconocido', 'pago', 'menci√≥n', 'a', 'educadoras', 'diferenciales', 'p√°rvulos', 'Cubillos', 'adelant√≥', 'Gobierno', 'dispuesto', 'a', 'discutir', 'nuevamente', 'punto', 'se√±alando', 'queda', 'conforme', 'planteado', 'petitorio', 'd√≠a', 'pasa', 'entrampamiento', 'demoremos', 'empezar', 'a', 'avanzar', 'puntos', 'acuerdo', 'critic√≥', 'Cubillos', 'llamado', 'directo', 'a', 'colegio', '5', 'siguen', 'paro', 'a', 'ojal√°', 'depongan', 'Aguilar', 'alcance', 'paro', 'docente', 'Mostr√≥', 'organizaci√≥n', 'enfrenta', 'a', 'gobiernos', 'empresariales', 'Ministerio', 'energ√≠as', 'puestas', 'planes', 'recuperaci√≥n', 'reparar', 'da√±o', 'produce', 'a', 'ni√±os', 'educaci√≥n', 'publica', 'paralizaci√≥n', 'puntos', 'exig√≠a', 'Colegio', 'Profesores', 'primeras', 'semanas', 'movilizaci√≥n', 'ministra', 'participara', 'negociaciones', 'instancia', 'lideradas', 'subsecretario', 'l√≠nea', 'descart√≥', 'mea', 'culpa', 'sumarse', 'a', 'mesas', 'reci√©n', 'cuarta', 'semana', 'paro', 'gobierno', 'actuado', 'predisposici√≥n', 'posici√≥n', 'directiva', 'Colegio', 'Profesores', 'hablar', 'subsecrterario', 'gobierno', 'actuado', 'opini√≥n', 'A', 'problema', 'conversado', 'Colegio', 'Profesores', 'Escucha', 'entrevista', 'completa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-26T19:41:36.055210Z",
          "start_time": "2019-08-26T19:41:36.051221Z"
        },
        "id": "oflTH9xfSQHl"
      },
      "source": [
        "### Extracci√≥n de Frases\n",
        "\n",
        "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`).\n",
        "\n",
        "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:18:18.064454Z",
          "start_time": "2020-05-07T19:18:03.208281Z"
        },
        "id": "vT6jB99sSQHl"
      },
      "source": [
        "# Phrases recibe una lista de oraciones, y junta bigramas que est√©n al menos 100 veces repetidos\n",
        "# como un √∫nico token. Detr√°s de esto hay un modelo estad√≠stico basado en frecuencias, probabilidades, etc\n",
        "# pero en t√©rminos simples ese es el resultado\n",
        "\n",
        "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHZ8f4bbSQHl"
      },
      "source": [
        "Ahora, usamos `Phraser` para re-tokenizamos el corpus con los bigramas encontrados. Es decir, juntamos los tokens separados que detectamos como frases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:20:57.432220Z",
          "start_time": "2020-05-07T19:20:13.544388Z"
        },
        "id": "qsMhI7cDSQHm"
      },
      "source": [
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[cleaned_content]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:21:03.964097Z",
          "start_time": "2020-05-07T19:21:03.958113Z"
        },
        "id": "lyJJ_XOKSQHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23dbbd88-f942-4277-dd34-79ba9ccad5cb"
      },
      "source": [
        "# para ver como quedan las noticias retokenizadas, quitar comentario a la siguiente linea:\n",
        "print(sentences[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Polic√≠a', 'busca', 'a', 'mujer', 'acusada', 'matar', 'a', 'padre', 'discusi√≥n', 'venta', 'vivienda', 'Pudahuel', 'Detectives', 'Polic√≠a_Investigaciones', 'realizan', 'peritajes', 'detener', 'a', 'mujer', '45', 'a√±os', 'presunta', 'responsabilidad', 'ataque', 'arma', 'cortante', 'padre', 'caus√≥', 'muerte', 'comuna', 'Pudahuel', 'ocurri√≥', 'calle', 'Presidente', 'Truman', 'intersecci√≥n', 'Teniente', 'Cruz', 'acorde', 'a', 'declaraci√≥n', 'hijo', 'v√≠ctima', 'hermano', 'victimaria', 'sostuvieron', 'enfrentamiento', 'verbal', 'a', 'intensi√≥n', 'Hernan', 'Silva', 'P√©rez', 'vender', 'casa', 'Negocio', 'causado', 'molestia', 'hija', 'Tania', 'Silva', 'discusi√≥n', 'acudido', 'a', 'cocina', 'vivienda', 'volver', 'cuchillo', 'apu√±alar', 'a', 'padre', 'primeras', 'diligencias', 'realizaron', 'carabineros', '45¬∫', 'comisar√≠a', 'tomaron', 'declaraci√≥n', '√∫nico', 'testigo', 'crimen', 'interior', 'vivienda', 'capit√°n', 'Carlos', 'Lagos', 'principal', 'hip√≥tesis', 'apunta', 'a', 'discusi√≥n', 'dinero', 'venta', 'inmueble', 'detectives', 'Brigada_Homicidios', 'PDI', 'realizaron', 'peritajes', 'corroborando', 'v√≠ctima', 'muri√≥', 'apu√±alado', 'ocasi√≥n', 'a', 'altura', 't√≥rax', 'subcomisario', 'Cristi√°n', 'Tur', 'presunta', 'responsable', 'identificada', 'raz√≥n', 'diligencias', 'polic√≠a', 'enfocadas', 'paradero', 'detener', 'a', 'Tania', 'Silva', 'Herrera', '45', 'a√±os', 'acorde', 'a', 'expresado', 'familiares', 'vivir√≠a', 'situaci√≥n_calle', 'buscada', 'delito', 'parricidio', 'art√≠culo_describe', 'proceso_judicial', 'curso_posibilidad', 'cargos_desestimados', 'finalizar_investigaci√≥n', 'considerar_imputados', 'culpables_Justicia', 'dicte_sentencia', 'Art√≠culo_04', 'C√≥digo_Procesal', 'Penal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-26T20:46:14.842693Z",
          "start_time": "2019-08-26T20:46:14.839706Z"
        },
        "id": "srh235fDSQHm"
      },
      "source": [
        "### Definir el modelo\n",
        "\n",
        "\n",
        "\n",
        "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
        "\n",
        "Algunos par√°metros importantes:\n",
        "\n",
        "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada.\n",
        "- `window` : Tama√±o de la ventana. Usaremos 4.\n",
        "- `size` : El tama√±o de los embeddings que crearemos. Por lo general, el rendimiento sube cuando se usan mas dimensiones, pero despu√©s de 300 ya no se nota cambio. Ahora, usaremos solo 200.\n",
        "- `workers`: Cantidad de CPU que ser√°n utilizadas en el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T15:29:43.525865Z",
          "start_time": "2020-05-07T15:29:43.521840Z"
        },
        "id": "PxbhQixYSQHm"
      },
      "source": [
        "biobio_w2v = Word2Vec(min_count=10,\n",
        "                      window=4,\n",
        "                      vector_size=200,\n",
        "                      sample=6e-5,\n",
        "                      alpha=0.03,\n",
        "                      min_alpha=0.0007,\n",
        "                      negative=20,\n",
        "                      workers=multiprocessing.cpu_count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Word2Vec()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL5xfnTBOszO",
        "outputId": "0bf9010c-4fd0-4a77-c668-adf634a3634c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7fbc572eb550>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcoiPxJySQHn"
      },
      "source": [
        "### Construir el vocabulario\n",
        "\n",
        "Para esto, se crear√° un conjunto que contendr√° (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T15:30:09.864087Z",
          "start_time": "2020-05-07T15:29:45.813793Z"
        },
        "id": "8Gx_MtzeSQHn"
      },
      "source": [
        "biobio_w2v.build_vocab(sentences, progress_per=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-26T21:11:58.054500Z",
          "start_time": "2019-08-26T21:11:58.050511Z"
        },
        "id": "UGvA3Lr9SQHo"
      },
      "source": [
        "### Entrenar el Modelo\n",
        "\n",
        "A continuaci√≥n, entenaremos el modelo.\n",
        "Los par√°metros que usaremos ser√°n:\n",
        "\n",
        "- `total_examples`: N√∫mero de documentos.\n",
        "- `epochs`: N√∫mero de veces que se iterar√° sobre el corpus.\n",
        "\n",
        "Es recomendable que tengan instalado `cpython` antes de continuar. Aumenta bastante la velocidad de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T15:34:44.255083Z",
          "start_time": "2020-05-07T15:30:12.141203Z"
        },
        "scrolled": true,
        "id": "SXER4JiiSQHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "631e7c0b-8694-4d11-a47f-eb4ca8d1d7b1"
      },
      "source": [
        "t = time()\n",
        "biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=5, report_delay=10)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to train the model: 2.86 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMSGhv3XSQHo"
      },
      "source": [
        "Ahora que terminamos de entrenar el modelo, le indicamos que no lo entrenaremos mas.\n",
        "Esto nos permitir√° ejecutar eficientemente las tareas que realizaremos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T15:34:46.358716Z",
          "start_time": "2020-05-07T15:34:46.331779Z"
        },
        "id": "PWbHUPjJSQHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb16743f-1f92-4369-c1d6-957fea54bd93"
      },
      "source": [
        "biobio_w2v.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-8d993e9d3e98>:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  biobio_w2v.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-26T21:43:36.571382Z",
          "start_time": "2019-08-26T21:43:36.567392Z"
        },
        "id": "g47OlSg2SQHo"
      },
      "source": [
        "###  Guardar y cargar el modelo\n",
        "\n",
        "Para ahorrar tiempo, usaremos un modelo preentrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:23:19.548567Z",
          "start_time": "2020-05-07T19:23:18.572531Z"
        },
        "id": "hLd1DGPXSQHp"
      },
      "source": [
        "# Si entrenaste el modelo y lo quieres guardar, descomentar el siguiente bloque.\n",
        "if not os.path.exists('./pretrained_models'):\n",
        "    os.mkdir('./pretrained_models')\n",
        "biobio_w2v.save('./pretrained_models/biobio_w2v.model')\n",
        "\n",
        "\n",
        "# cargar el modelo (si es que lo entrenaron desde local.)\n",
        "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T17:05:18.639618Z",
          "start_time": "2020-05-07T17:02:10.981724Z"
        },
        "id": "hEvKTMmGSQHp"
      },
      "source": [
        "# descargar el modelo desde github\n",
        "def read_model_from_github(url):\n",
        "    if not os.path.exists('./pretrained_models'):\n",
        "        os.mkdir('./pretrained_models')\n",
        "\n",
        "    r = requests.get(url)\n",
        "    filename = url.split('/')[-1]\n",
        "    with open('./pretrained_models/' + filename, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return True\n",
        "\n",
        "\n",
        "[\n",
        "    read_model_from_github(file) for file in [\n",
        "        'https://github.com/dccuchile/CC6205/releases/download/Data/biobio_w2v.model',\n",
        "    ]\n",
        "]\n",
        "# cargar el modelo (si es que lo entrenaron desde local.)\n",
        "biobio_w2v = KeyedVectors.load(\"./pretrained_models/biobio_w2v.model\", mmap='r')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oqfd5wkSQHp"
      },
      "source": [
        "## **Tasks: Palabras mas similares y Analog√≠as**\n",
        "\n",
        "### **Palabras mas similares**\n",
        "\n",
        "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la informaci√≥n contextual de las palabras en vectores.\n",
        "\n",
        "Y como cualquier objeto matem√°tico, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:28:33.343259Z",
          "start_time": "2020-05-07T19:28:33.262476Z"
        },
        "id": "D6wSyaJ6SQHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9c4a2d-c780-4782-972f-ecaf48c6a924"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"perro\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('gato', 0.7324815392494202),\n",
              " ('perrito', 0.7017662525177002),\n",
              " ('cachorro', 0.6726856231689453),\n",
              " ('canino', 0.6614428758621216),\n",
              " ('mascota', 0.6354753971099854),\n",
              " ('animal', 0.6341222524642944),\n",
              " ('gatito', 0.6259311437606812),\n",
              " ('felino', 0.622412919998169),\n",
              " ('perros', 0.6207762360572815),\n",
              " ('perra', 0.5834704041481018)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:28:59.154190Z",
          "start_time": "2020-05-07T19:28:59.146214Z"
        },
        "id": "wvvUcuNaSQHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19dd4178-cf9a-49ce-973a-c85e8b1fa323"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"Chile\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Latinoam√©rica', 0.48114773631095886),\n",
              " ('pa√≠s', 0.4791499078273773),\n",
              " ('exportadores', 0.4531676173210144),\n",
              " ('chileno', 0.4481710195541382),\n",
              " ('Crecimiento', 0.4425714910030365),\n",
              " ('posiciona', 0.44039905071258545),\n",
              " ('Per√∫', 0.43912073969841003),\n",
              " ('Bolivia', 0.4357107877731323),\n",
              " ('chilenas', 0.43546050786972046),\n",
              " ('chilena', 0.430389940738678)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:29:49.464492Z",
          "start_time": "2020-05-07T19:29:49.432164Z"
        },
        "id": "M8UMSL20SQHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf749477-5d63-4ec1-be1b-7f19bfa08fb7"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"Bolsonaro\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ultraderechista_Jair', 0.7692869305610657),\n",
              " ('excapit√°n_Ej√©rcito', 0.7673073410987854),\n",
              " ('Jair_Bolsonaro', 0.7401224374771118),\n",
              " ('Haddad', 0.6947491765022278),\n",
              " ('exmilitar', 0.6393497586250305),\n",
              " ('Brasil', 0.618027925491333),\n",
              " ('Brasilia', 0.6159982681274414),\n",
              " ('nost√°lgico', 0.6144426465034485),\n",
              " ('Fernando_Haddad', 0.606082558631897),\n",
              " ('Ultraderechista', 0.603746771812439)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:29:56.028795Z",
          "start_time": "2020-05-07T19:29:56.014832Z"
        },
        "id": "x8yQ_thcSQHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccdc7a4-dddc-4c72-b6d8-7d901539a21d"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"Trump\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Casa_Blanca', 0.709988534450531),\n",
              " ('Donald_Trump', 0.7093803882598877),\n",
              " ('mandatario_estadounidense', 0.7046915292739868),\n",
              " ('inquilino', 0.678024411201477),\n",
              " ('administraci√≥n_Trump', 0.6302800178527832),\n",
              " ('Bolton', 0.6201831102371216),\n",
              " ('Unidos', 0.6197685599327087),\n",
              " ('Washington', 0.6131006479263306),\n",
              " ('presidente_estadounidense', 0.6014897227287292),\n",
              " ('Unidos_Donald', 0.5976207852363586)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:32:29.842609Z",
          "start_time": "2020-05-07T19:32:29.829644Z"
        },
        "scrolled": true,
        "id": "zDe2KBajSQHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a31cba9-f391-4a57-8efa-9675d3eb88b3"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"Pizza\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hut', 0.9047655463218689),\n",
              " ('Telepizza', 0.8243373036384583),\n",
              " ('ATT', 0.7196983098983765),\n",
              " ('Linio', 0.6911829113960266),\n",
              " ('Homecenter', 0.6828977465629578),\n",
              " ('Unimarc', 0.6740168333053589),\n",
              " ('Natura', 0.6694700717926025),\n",
              " ('Nestl√©', 0.6646097302436829),\n",
              " ('Coffee', 0.6622934937477112),\n",
              " ('Sodimac', 0.6596288681030273)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:32:39.355210Z",
          "start_time": "2020-05-07T19:32:39.348228Z"
        },
        "id": "8e1_voDvSQHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225f2807-bb3b-483d-dece-e352fddbdabc"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"pizza\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hamburguesas', 0.6893606185913086),\n",
              " ('cafeter√≠as', 0.6606759428977966),\n",
              " ('degustaciones', 0.6389386653900146),\n",
              " ('repartidor', 0.6389126777648926),\n",
              " ('pizzas', 0.6339085102081299),\n",
              " ('platos', 0.6290442943572998),\n",
              " ('arroz', 0.6215819716453552),\n",
              " ('hamburguesa', 0.6195734739303589),\n",
              " ('vodka', 0.6181467771530151),\n",
              " ('chocolates', 0.616828441619873)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:32:59.165524Z",
          "start_time": "2020-05-07T19:32:59.153557Z"
        },
        "id": "862JVl0RSQHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96bfdacb-3782-452c-ccb6-901da019a961"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"Uber\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Cabify', 0.7779495716094971),\n",
              " ('Eats', 0.700801432132721),\n",
              " ('DiDi', 0.676181435585022),\n",
              " ('Rappi', 0.6054359078407288),\n",
              " ('Conductores', 0.5858488082885742),\n",
              " ('aplicaciones', 0.5700352787971497),\n",
              " ('Beat', 0.566635251045227),\n",
              " ('app', 0.5640519857406616),\n",
              " ('choferes', 0.5415353178977966),\n",
              " ('taxistas', 0.5351073145866394)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:33:34.899941Z",
          "start_time": "2020-05-07T19:33:34.885980Z"
        },
        "id": "lexHPPjdSQHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b7b81d-3e24-4b02-8331-1b6cc085217a"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"Huawei\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ZTE', 0.7025798559188843),\n",
              " ('telecomunicaciones', 0.639496922492981),\n",
              " ('Wanzhou', 0.617560863494873),\n",
              " ('5G', 0.6000877022743225),\n",
              " ('Meng', 0.5700865387916565),\n",
              " ('Pekin', 0.5595781803131104),\n",
              " ('Ren', 0.5414323210716248),\n",
              " ('gigante_asi√°tico', 0.5363011956214905),\n",
              " ('sanciones_estadounidenses', 0.5254279375076294),\n",
              " ('China', 0.5246084928512573)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:33:41.167456Z",
          "start_time": "2020-05-07T19:33:41.155482Z"
        },
        "id": "5T8ZPF1KSQHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb85404f-a58a-4c74-da59-5e59051d95ca"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"TVN\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Televisi√≥n', 0.6506631374359131),\n",
              " ('directorio', 0.5564517974853516),\n",
              " ('Orrego', 0.48779889941215515),\n",
              " ('C5N', 0.47816959023475647),\n",
              " ('Panorama', 0.47550544142723083),\n",
              " ('Chilevisi√≥n', 0.46353694796562195),\n",
              " ('cupr√≠fera', 0.4621235728263855),\n",
              " ('matinal', 0.45264914631843567),\n",
              " ('CHV', 0.4483409821987152),\n",
              " ('Mega', 0.44625139236450195)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:33:47.881474Z",
          "start_time": "2020-05-07T19:33:47.871502Z"
        },
        "id": "0lE859LiSQHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7294f1-4152-4fd3-a070-bdbd335d10c5"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"ultraderechista\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('excapit√°n_Ej√©rcito', 0.7039800882339478),\n",
              " ('ultraderecha', 0.7019724249839783),\n",
              " ('ultraderechista_Jair', 0.661270022392273),\n",
              " ('extrema_derecha', 0.6610966324806213),\n",
              " ('Finlandeses', 0.659309983253479),\n",
              " ('Verdaderos', 0.6557668447494507),\n",
              " ('antiinmigraci√≥n', 0.6375353336334229),\n",
              " ('exmilitar', 0.636165201663971),\n",
              " ('Liga', 0.6324677467346191),\n",
              " ('Matteo_Salvini', 0.6320110559463501)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:33:55.044392Z",
          "start_time": "2020-05-07T19:33:55.033420Z"
        },
        "id": "Hjd9Hn6gSQHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b3b252-6894-4cdb-be92-bdf1a85387d9"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"tonto\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('est√∫pido', 0.7166074514389038),\n",
              " ('loco', 0.7140238285064697),\n",
              " ('¬øQue', 0.7014246582984924),\n",
              " ('Dije', 0.6799631714820862),\n",
              " ('suena', 0.6779486536979675),\n",
              " ('pinta', 0.6729979515075684),\n",
              " ('har√©', 0.6724047660827637),\n",
              " ('repugnante', 0.6709104180335999),\n",
              " ('dices', 0.6700376868247986),\n",
              " ('ego√≠sta', 0.6675840616226196)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWr76TLqSQHr"
      },
      "source": [
        "### **Analog√≠as**\n",
        "\n",
        "Por otra parte, la analog√≠a consiste en comparar 3 terminos mediante una operaci√≥n del estilo:\n",
        "\n",
        "$$palabra1 - palabra2 \\approx palabra 3 - x$$\n",
        "\n",
        "para encontrar relaciones entre estos.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "| palabra 1 (pos) |  palabra 2 (neg) |\n",
        "|-----------------|------------------|\n",
        "|  macri          | Argentina          |\n",
        "| Brasil           |  x               |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:36:24.480150Z",
          "start_time": "2020-05-07T19:36:24.462198Z"
        },
        "id": "cHMLglR7SQHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c505bd-2da4-4515-8860-5f134fa4912e"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"Macri\", \"Brasil\"], negative=['Argentina'], topn=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Bolsonaro', 0.6208245158195496),\n",
              " ('ultraderechista_Jair', 0.5999900698661804),\n",
              " ('Jair_Bolsonaro', 0.5594528913497925),\n",
              " ('Michel_Temer', 0.5592767596244812),\n",
              " ('excapit√°n_Ej√©rcito', 0.556822657585144),\n",
              " ('Haddad', 0.532974123954773),\n",
              " ('Fernando_Haddad', 0.5272845029830933),\n",
              " ('Temer', 0.5139399766921997),\n",
              " ('Brasilia', 0.4959222972393036),\n",
              " ('Ultraderechista', 0.47941094636917114)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:36:10.444907Z",
          "start_time": "2020-05-07T19:36:10.435930Z"
        },
        "id": "FLaLGvm4SQHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec1271b-821e-456a-8530-8816716063bd"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"Chile\", \"Huawei\"], negative=['China'], topn=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Falabella', 0.47030484676361084),\n",
              " ('portabilidad', 0.4300999641418457),\n",
              " ('Multicaja', 0.4203709363937378),\n",
              " ('CMR', 0.41495582461357117),\n",
              " ('Sodimac', 0.4064332842826843),\n",
              " ('Valledor', 0.3986559212207794),\n",
              " ('Telef√≥nica', 0.39562681317329407),\n",
              " ('Ciberseguridad', 0.39511701464653015),\n",
              " ('Subtel', 0.3942727744579315),\n",
              " ('retail', 0.39312079548835754)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc4GN6oSU__F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5369df-deb3-49c3-ac86-f19261392483"
      },
      "source": [
        "biobio_w2v.wv.most_similar(positive=[\"perro\", \"tibur√≥n\"], negative=['gato'], topn=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('canguro', 0.6266387104988098),\n",
              " ('domador', 0.6069827079772949),\n",
              " ('reptil', 0.6003487706184387),\n",
              " ('pez', 0.5977956652641296),\n",
              " ('jirafa', 0.5955661535263062),\n",
              " ('le√≥n', 0.5919601321220398),\n",
              " ('oso', 0.5904221534729004),\n",
              " ('macho', 0.5821169018745422),\n",
              " ('foca', 0.5812079310417175),\n",
              " ('serpiente', 0.5803099870681763)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-28T19:55:09.459852Z",
          "start_time": "2019-08-28T19:55:09.454865Z"
        },
        "id": "z3bDLWKpSQHu"
      },
      "source": [
        "## **Word Embeddings como caracter√≠sticas para clasificar**\n",
        "\n",
        "\n",
        "En esta secci√≥n, veremos como utilizar los word embeddings como caracter√≠stica para **clasificar nuevamente el t√≥pico de las noticias de la radio biobio**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hlZvK45SQHv"
      },
      "source": [
        "Primero, obtendremos los datos y sus categor√≠as y dejamos solo las primeras 20:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz8BRfleSQHv"
      },
      "source": [
        "### Cargar el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:44:39.200212Z",
          "start_time": "2020-05-07T19:44:39.176275Z"
        },
        "id": "UmKhH43ESQHv"
      },
      "source": [
        "dataset = dataset_r.copy(deep=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:44:45.081609Z",
          "start_time": "2020-05-07T19:44:45.065652Z"
        },
        "id": "bnVQyXXkSQHv"
      },
      "source": [
        "# creamos una nueva columna titulo y contenido.\n",
        "content = dataset['content']\n",
        "\n",
        "# obtenemos las clases\n",
        "subcategory = dataset.subcategory\n",
        "\n",
        "# dejamos en el dataset solo contenido de la noticia y categoria\n",
        "dataset = pd.DataFrame({'content': content, 'category': subcategory})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:44:58.537303Z",
          "start_time": "2020-05-07T19:44:58.486439Z"
        },
        "id": "VP9lR3lGSQHv"
      },
      "source": [
        "NUM_SAMPLES = 250\n",
        "\n",
        "categorias_seleccionadas = [\n",
        "    'america-latina', 'eeuu', 'europa', 'chile', 'region-metropolitana',\n",
        "    'region-del-bio-bio', 'negocios-y-empresas', 'region-de-los-lagos',\n",
        "    'actualidad-economica', 'region-de-valparaiso', 'region-de-la-araucania',\n",
        "    'curiosidades', 'asia', 'region-de-los-rios', 'entrevistas', 'debates',\n",
        "    'mediooriente', 'viral', 'animales', 'tu-bolsillo'\n",
        "]\n",
        "\n",
        "# filtrar solo categorias seleccionadas\n",
        "dataset = dataset[dataset['category'].isin(categorias_seleccionadas)]\n",
        "\n",
        "# balancear clases\n",
        "g = dataset.groupby('category')\n",
        "dataset = pd.DataFrame(\n",
        "    g.apply(lambda x: x.sample(NUM_SAMPLES).reset_index(drop=True))\n",
        ").reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELrZh9ZHSQHv"
      },
      "source": [
        "Ahora, transformamos cada documento del dataset en el promedio de sus embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-23T20:43:25.817110Z",
          "start_time": "2019-09-23T20:43:25.797172Z"
        },
        "id": "zD_A7ogRSQHw"
      },
      "source": [
        "### Dividir el dataset en training y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:45:05.304455Z",
          "start_time": "2020-05-07T19:45:05.296477Z"
        },
        "id": "y3pOTi67SQHw"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
        "                                                    dataset.category,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNtnEzWMSQHw"
      },
      "source": [
        "Primero, crearemos el Transformer con el cual convertiremos el documento a vector. (puede que les sirva para la competencia...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spUfFXrNSQHw"
      },
      "source": [
        "### Doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-17T20:42:22.105698Z",
          "start_time": "2020-05-17T20:42:22.094754Z"
        },
        "id": "IXq9b_pYSQHw"
      },
      "source": [
        "class Doc2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Transforma tweets a representaciones vectoriales usando alg√∫n modelo de Word Embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, aggregation_func):\n",
        "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
        "        self.model = model.wv\n",
        "\n",
        "        # indicamos la funci√≥n de agregaci√≥n (np.min, np.max, np.mean, np.sum, ...)\n",
        "        self.aggregation_func = aggregation_func\n",
        "\n",
        "    def simple_tokenizer(self, doc, lower=False):\n",
        "        \"\"\"Tokenizador. Elimina signos de puntuaci√≥n, lleva las letras a min√∫scula(opcional) y\n",
        "           separa el tweet por espacios.\n",
        "        \"\"\"\n",
        "        if lower:\n",
        "            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "\n",
        "        doc_embeddings = []\n",
        "\n",
        "        for doc in X:\n",
        "            # tokenizamos el documento. Se llevan todos los tokens a min√∫scula.\n",
        "            # ojo con esto, ya que puede que tokens con min√∫scula y may√∫scula tengan\n",
        "            # distintas representaciones\n",
        "            tokens = self.simple_tokenizer(doc, lower = True)\n",
        "\n",
        "            selected_wv = []\n",
        "            for token in tokens:\n",
        "                if token in self.model.index_to_key:\n",
        "                    selected_wv.append(self.model[token])\n",
        "\n",
        "            # si seleccionamos por lo menos un embedding para el tweet, lo agregamos y luego lo a√±adimos.\n",
        "            if len(selected_wv) > 0:\n",
        "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
        "                doc_embeddings.append(doc_embedding)\n",
        "            # si no, a√±adimos un vector de ceros que represente a ese documento.\n",
        "            else:\n",
        "                print('No pude encontrar ning√∫n embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n",
        "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo\n",
        "\n",
        "        return np.array(doc_embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3NG6AvpSQHw"
      },
      "source": [
        "### Definimos el pipeline\n",
        "\n",
        "\n",
        "Usaremos la transformaci√≥n que creamos antes mas una regresi√≥n log√≠stica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:50:34.393237Z",
          "start_time": "2020-05-07T19:50:34.387253Z"
        },
        "id": "eBjMxandSQHx"
      },
      "source": [
        "clf = LogisticRegression(max_iter=1000000)\n",
        "\n",
        "doc2vec_mean = Doc2VecTransformer(biobio_w2v, np.mean)\n",
        "doc2vec_sum = Doc2VecTransformer(biobio_w2v, np.sum)\n",
        "doc2vec_max = Doc2VecTransformer(biobio_w2v, np.max)\n",
        "\n",
        "\n",
        "pipeline = Pipeline([('doc2vec', doc2vec_sum), ('clf', clf)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:50:59.881521Z",
          "start_time": "2020-05-07T19:50:40.532331Z"
        },
        "id": "buMnvsO8SQHx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "360205f8-3b12-41fc-8e97-a599b35922a7"
      },
      "source": [
        "pipeline.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('doc2vec',\n",
              "                 Doc2VecTransformer(aggregation_func=<function sum at 0x7fbcb174ac10>,\n",
              "                                    model=<gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0>)),\n",
              "                ('clf', LogisticRegression(max_iter=1000000))])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;doc2vec&#x27;,\n",
              "                 Doc2VecTransformer(aggregation_func=&lt;function sum at 0x7fbcb174ac10&gt;,\n",
              "                                    model=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0&gt;)),\n",
              "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;doc2vec&#x27;,\n",
              "                 Doc2VecTransformer(aggregation_func=&lt;function sum at 0x7fbcb174ac10&gt;,\n",
              "                                    model=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0&gt;)),\n",
              "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Doc2VecTransformer</label><div class=\"sk-toggleable__content\"><pre>Doc2VecTransformer(aggregation_func=&lt;function sum at 0x7fbcb174ac10&gt;,\n",
              "                   model=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000000)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-23T20:43:47.337340Z",
          "start_time": "2019-09-23T20:43:47.317007Z"
        },
        "id": "SfiGZpotSQHx"
      },
      "source": [
        "**Predecimos y evaluamos:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:51:07.108644Z",
          "start_time": "2020-05-07T19:51:05.799089Z"
        },
        "id": "v4R7zhgqSQHx"
      },
      "source": [
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:51:12.601957Z",
          "start_time": "2020-05-07T19:51:12.589989Z"
        },
        "id": "DKEU3QgBSQHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42362fd2-9d5b-467f-9081-03ccd19fa076"
      },
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[43  0  0  0  9  0  0  1  1  0 22  0  0  0  0  3  0 12  0]\n",
            " [ 1 67  0  2  2  2  9  0  0  1  1  0  1  0  0  0  3  0  0]\n",
            " [ 0  1 55  7  0  3  2  0  0  0  2  0  0  0  0  0  0  0  5]\n",
            " [ 2  1  3 56  0  3  7  0  2  1  0  0  0  0  0  0  0  0  3]\n",
            " [10  3  0  0 53  0  0  5  0  0  6  2  1  1  5  3  5  6  1]\n",
            " [ 2  1  3  4  1 42  3  0  4  1  4  0  0  0  0  0  0  1 14]\n",
            " [ 1  2  1  4  0  3 66  0  2  5  0  0  0  0  0  0  0  0  2]\n",
            " [ 3  0  0  0  7  1  0 53  0  0  1  1  0  2  2  0  2  2  1]\n",
            " [ 0  5  1  7  0  2  4  1 65  0  1  0  0  0  0  0  1  0  1]\n",
            " [ 0  1  0  2  0  0  8  0  6 66  0  0  0  0  0  0  0  0  2]\n",
            " [19  0  0  0  3  1  0  2  0  0 35  0  2  0  0  3  1 11  1]\n",
            " [ 1  0  0  0  1  0  0  1  0  0  2 61  4  5  1  7  3  0  0]\n",
            " [ 0  0  0  0  1  0  0  0  0  0  2  1 59  8  4  1  1  0  0]\n",
            " [ 0  0  0  0  1  0  0  0  0  0  1  6  5 62  3  2  0  0  0]\n",
            " [ 2  1  1  1  2  0  0  3  0  0  0  2  4  0 40  9 11  0  0]\n",
            " [ 0  1  0  0  2  0  0  1  0  0  1  3  1  4  3 59  4  0  0]\n",
            " [ 1  0  1  0  3  1  0  1  0  0  1  2  2  1  7  3 60  0  0]\n",
            " [13  0  0  0  6  4  0  1  0  0 14  0  0  1  0  0  0 46  2]\n",
            " [ 0  0 10  1  2  9  4  1  0  0  0  0  0  0  1  0  1  0 45]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:51:18.863055Z",
          "start_time": "2020-05-07T19:51:18.805699Z"
        },
        "scrolled": false,
        "id": "jCBjfMFBSQHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd43980-f8f0-401e-c7cb-b066084bffc2"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "  actualidad-economica       0.44      0.47      0.46        91\n",
            "        america-latina       0.81      0.75      0.78        89\n",
            "              animales       0.73      0.73      0.73        75\n",
            "                  asia       0.67      0.72      0.69        78\n",
            "                 chile       0.57      0.52      0.55       101\n",
            "          curiosidades       0.59      0.53      0.56        80\n",
            "                  eeuu       0.64      0.77      0.70        86\n",
            "           entrevistas       0.76      0.71      0.73        75\n",
            "                europa       0.81      0.74      0.77        88\n",
            "          mediooriente       0.89      0.78      0.83        85\n",
            "   negocios-y-empresas       0.38      0.45      0.41        78\n",
            "region-de-la-araucania       0.78      0.71      0.74        86\n",
            "   region-de-los-lagos       0.75      0.77      0.76        77\n",
            "    region-de-los-rios       0.74      0.78      0.76        80\n",
            "  region-de-valparaiso       0.61      0.53      0.56        76\n",
            "    region-del-bio-bio       0.66      0.75      0.70        79\n",
            "  region-metropolitana       0.65      0.72      0.69        83\n",
            "           tu-bolsillo       0.59      0.53      0.56        87\n",
            "                 viral       0.58      0.61      0.60        74\n",
            "\n",
            "              accuracy                           0.66      1568\n",
            "             macro avg       0.67      0.66      0.66      1568\n",
            "          weighted avg       0.66      0.66      0.66      1568\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T19:51:25.139392Z",
          "start_time": "2020-05-07T19:51:25.132412Z"
        },
        "id": "gdTu7vzmSQHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5822dfa8-c27f-4be4-f54f-2071f12eba8d"
      },
      "source": [
        "pipeline.predict(\n",
        "    [(\"Alguna noticia..\")])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['region-del-bio-bio'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-24T12:57:03.817026Z",
          "start_time": "2019-09-24T12:57:03.814008Z"
        },
        "id": "uBlia-JJSQHy"
      },
      "source": [
        "## **Usandolo con BoW**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLBZbmSShn17"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
        "                                                    dataset.category,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T16:39:09.997634Z",
          "start_time": "2020-05-07T16:39:09.992647Z"
        },
        "id": "IZ6IO16JSQHy"
      },
      "source": [
        "# Definimos el vectorizador para convertir el texto a BoW:\n",
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
        "\n",
        "# Definimos el clasificador que usaremos.\n",
        "clf_2 = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Definimos el pipeline\n",
        "pipeline_2 = Pipeline([('features',\n",
        "                        FeatureUnion([('bow', CountVectorizer()),\n",
        "                                      ('doc2vec', doc2vec_sum)])), ('clf', clf)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T16:44:04.514995Z",
          "start_time": "2020-05-07T16:39:13.770150Z"
        },
        "id": "TnTlbg6FSQHy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "4a693358-8a93-4a16-9f8c-9b950b7c2440"
      },
      "source": [
        "pipeline_2.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('features',\n",
              "                 FeatureUnion(transformer_list=[('bow', CountVectorizer()),\n",
              "                                                ('doc2vec',\n",
              "                                                 Doc2VecTransformer(aggregation_func=<function sum at 0x7fbcb174ac10>,\n",
              "                                                                    model=<gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0>))])),\n",
              "                ('clf', LogisticRegression(max_iter=1000000))])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
              "                 FeatureUnion(transformer_list=[(&#x27;bow&#x27;, CountVectorizer()),\n",
              "                                                (&#x27;doc2vec&#x27;,\n",
              "                                                 Doc2VecTransformer(aggregation_func=&lt;function sum at 0x7fbcb174ac10&gt;,\n",
              "                                                                    model=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0&gt;))])),\n",
              "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
              "                 FeatureUnion(transformer_list=[(&#x27;bow&#x27;, CountVectorizer()),\n",
              "                                                (&#x27;doc2vec&#x27;,\n",
              "                                                 Doc2VecTransformer(aggregation_func=&lt;function sum at 0x7fbcb174ac10&gt;,\n",
              "                                                                    model=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0&gt;))])),\n",
              "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">features: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;bow&#x27;, CountVectorizer()),\n",
              "                               (&#x27;doc2vec&#x27;,\n",
              "                                Doc2VecTransformer(aggregation_func=&lt;function sum at 0x7fbcb174ac10&gt;,\n",
              "                                                   model=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0&gt;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bow</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>doc2vec</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Doc2VecTransformer</label><div class=\"sk-toggleable__content\"><pre>Doc2VecTransformer(aggregation_func=&lt;function sum at 0x7fbcb174ac10&gt;,\n",
              "                   model=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7fbca86be9d0&gt;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000000)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T16:44:08.965415Z",
          "start_time": "2020-05-07T16:44:07.164065Z"
        },
        "id": "FVL6-TGWSQHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9bfdaae-01e7-48ed-a957-e4097982d014"
      },
      "source": [
        "y_pred_2 = pipeline_2.predict(X_test)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_2)\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[55  0  0  0  8  0  1  1  1  0 15  0  0  0  0  2  0  8  0]\n",
            " [ 1 68  0  1  2  2  6  0  3  0  1  0  0  0  0  1  3  0  1]\n",
            " [ 0  3 62  3  0  2  0  0  0  0  0  0  0  0  0  0  0  0  5]\n",
            " [ 1  1  1 61  0  3  4  0  1  2  0  0  0  0  0  0  0  0  4]\n",
            " [ 5  4  1  0 61  0  0  5  1  0  2  3  2  0  4  2  5  6  0]\n",
            " [ 0  1  4  3  0 48  3  0  2  0  3  0  0  0  0  0  0  1 15]\n",
            " [ 1  2  1  5  0  2 67  0  2  5  0  0  0  0  0  0  0  0  1]\n",
            " [ 3  0  0  0  2  0  0 64  0  0  1  1  0  0  1  0  0  3  0]\n",
            " [ 1  2  1  3  0  2  4  0 73  1  0  0  0  0  0  0  0  1  0]\n",
            " [ 0  0  0  3  0  1  5  0  3 72  0  0  0  0  0  0  0  0  1]\n",
            " [11  0  0  0  2  2  0  2  0  0 49  1  1  0  0  2  1  7  0]\n",
            " [ 0  0  0  0  1  0  0  1  0  0  0 68  3  6  2  5  0  0  0]\n",
            " [ 0  0  0  0  1  0  0  0  0  0  2  1 67  4  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  1  3 71  4  1  0  0  0]\n",
            " [ 1  0  1  0  1  0  0  1  0  0  1  0  5  0 59  5  2  0  0]\n",
            " [ 0  0  0  0  1  1  0  0  0  0  0  0  1  2  2 70  1  0  1]\n",
            " [ 1  0  0  0  6  1  0  1  0  0  2  1  2  0  3  1 65  0  0]\n",
            " [ 8  0  0  0  7  5  0  0  0  0 13  0  0  0  1  0  0 53  0]\n",
            " [ 0  0  9  1  1  9  1  0  1  0  0  0  0  0  0  0  1  0 51]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-07T16:44:11.609183Z",
          "start_time": "2020-05-07T16:44:11.553969Z"
        },
        "id": "XILZeUSwSQHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690136db-cfa7-43c7-eb49-875f75e3f880"
      },
      "source": [
        "print(classification_report(y_test, y_pred_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "  actualidad-economica       0.62      0.60      0.61        91\n",
            "        america-latina       0.84      0.76      0.80        89\n",
            "              animales       0.78      0.83      0.80        75\n",
            "                  asia       0.76      0.78      0.77        78\n",
            "                 chile       0.66      0.60      0.63       101\n",
            "          curiosidades       0.62      0.60      0.61        80\n",
            "                  eeuu       0.74      0.78      0.76        86\n",
            "           entrevistas       0.85      0.85      0.85        75\n",
            "                europa       0.84      0.83      0.83        88\n",
            "          mediooriente       0.90      0.85      0.87        85\n",
            "   negocios-y-empresas       0.55      0.63      0.59        78\n",
            "region-de-la-araucania       0.89      0.79      0.84        86\n",
            "   region-de-los-lagos       0.80      0.87      0.83        77\n",
            "    region-de-los-rios       0.86      0.89      0.87        80\n",
            "  region-de-valparaiso       0.77      0.78      0.77        76\n",
            "    region-del-bio-bio       0.78      0.89      0.83        79\n",
            "  region-metropolitana       0.83      0.78      0.81        83\n",
            "           tu-bolsillo       0.67      0.61      0.64        87\n",
            "                 viral       0.65      0.69      0.67        74\n",
            "\n",
            "              accuracy                           0.76      1568\n",
            "             macro avg       0.76      0.76      0.76      1568\n",
            "          weighted avg       0.76      0.76      0.76      1568\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeVEtswQSQH0"
      },
      "source": [
        "### Propuesto...\n",
        "\n",
        "- Usar su modelo de embeddings favorito para ver si mejora la clasificaci√≥n:\n",
        "    \n",
        " - Fast y word2vec en espa√±ol, [cortes√≠a](https://github.com/dccuchile/spanish-word-embeddings) de los grandes del DCC\n",
        " - [Conceptnet](https://github.com/commonsense/conceptnet-numberbatch)\n",
        "\n",
        "\n",
        "- Visualizar los documentos usando `doc2vec`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2DyCzBUbfwk"
      },
      "source": [
        "## Clasificaci√≥n de texto usando CNN + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HylpfYQcADP"
      },
      "source": [
        "Para este caso vamos a trabajar con un dataset de noticias, el cual es f√°cilmente descargable con la librer√≠a y da muchos mejores resultados (ya que los anteriores estaban ah√≠ nomas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTn7dZVwq-SO",
        "outputId": "c10c3966-1d64-40c0-faed-dd692d4307a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "# Instalamos portalocker para acceder a los datasets de Pytorch\n",
        "!pip install portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZgbo70IcGrq"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/text/stable/datasets.html#ag-news\n",
        "import os\n",
        "import torch\n",
        "from random import choice\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "train_dataset, test_dataset = AG_NEWS(root=\"data\", split=('train', 'test'))\n",
        "train_list = list(train_dataset)\n",
        "test_list = list(test_dataset)\n",
        "\n",
        "# Informacion relevante del dataset\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(tokenizer(x[1]) for x in train_list)\n",
        "\n",
        "vocab.set_default_index(0)\n",
        "vocab.insert_token('<pad>', 1)\n",
        "\n",
        "stoi = vocab.get_stoi()\n",
        "\n",
        "num_classes = 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTNLFNbTc5rx"
      },
      "source": [
        "Luego, creamos una red no tan profunda pero bien competente para nuestra tarea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOwf8CZZctxG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from itertools import zip_longest\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=10,\n",
        "                 cnn_pool_channels=24, cnn_kernel_size=3):\n",
        "\n",
        "        # Inicializamos la clase padre\n",
        "        super().__init__()\n",
        "\n",
        "        # Creamos la capa de embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Creamos la capa de convoluci√≥n\n",
        "        # `in_channels`: Es el n√∫mero de canales de entrada de la convoluci√≥n. En este caso, como estamos trabajando con texto, s√≥lo tenemos un canal, por lo que `in_channels=1`.\n",
        "        # `out_channels`: Es el n√∫mero de canales de salida de la convoluci√≥n. Especifica la cantidad de filtros que se aplicar√°n a la entrada. En este caso, queremos generar `cnn_pool_channels` canales de salida, por lo que `out_channels=cnn_pool_channels`.\n",
        "        # `kernel_size`: Es el tama√±o del kernel de la convoluci√≥n. En este caso, estamos usando un kernel de tama√±o `cnn_kernel_size * embed_dim`, donde `embed_dim` es la dimensi√≥n de los vectores de embedding. Esto significa que cada filtro de la convoluci√≥n cubrir√° `cnn_kernel_size` palabras (o tokens) en una dimensi√≥n y `embed_dim` en la otra.\n",
        "        # `stride`: Es el desplazamiento que se aplica a la entrada de la convoluci√≥n. En este caso, estamos desplazando la entrada `embed_dim` unidades en cada paso. Esto significa que se aplicar√°n filtros a cada palabra (o token) de la entrada.\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=cnn_pool_channels,\n",
        "            kernel_size=cnn_kernel_size * embed_dim,\n",
        "            stride=embed_dim,\n",
        "        )\n",
        "\n",
        "        # Calculamos el tama√±o de entrada de la capa lineal\n",
        "        fc_in_size = cnn_pool_channels\n",
        "\n",
        "        # Creamos la capa lineal\n",
        "        self.fc = nn.Linear(fc_in_size, num_classes)\n",
        "\n",
        "        # Inicializamos los pesos de las capas\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Definimos el rango de los valores iniciales de los pesos\n",
        "        initrange = 0.5\n",
        "\n",
        "        # Inicializamos los pesos de la capa de embedding\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Inicializamos los pesos de la capa lineal\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Inicializamos los sesgos de la capa lineal en cero\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "\n",
        "        # Preparamos el input de la capa de embeddings a partir de text y offsets\n",
        "        text = torch.tensor(\n",
        "            list(\n",
        "                zip(\n",
        "                    *zip_longest(\n",
        "                        *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(texts)]]),\n",
        "                        fillvalue=vocab[\"<pad>\"]\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        ).to(text.device)\n",
        "\n",
        "        # Obtenemos la representaci√≥n de la frase a partir de la capa de embedding\n",
        "        h = self.embedding(text)\n",
        "\n",
        "        # Aplicamos la capa de convoluci√≥n\n",
        "        h = h.view(h.size(0), 1, -1)\n",
        "        h = torch.relu(self.conv(h))\n",
        "        h = h.mean(dim=2)\n",
        "\n",
        "        # Obtenemos el resultado final a partir de la capa lineal\n",
        "        output = self.fc(h)\n",
        "\n",
        "        # Aplicamos la funci√≥n de activaci√≥n log-softmax\n",
        "        return F.log_softmax(output, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLOXQIhec4PY"
      },
      "source": [
        "Finalmente, generamos la funci√≥n para cargar por batch y luego entrenamos directamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3HqZeLxA1q5",
        "outputId": "0a14186e-eb70-4139-a924-f5de11dffc75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Epoch: 001\t Phase: train Iter: 469/469\t iter-Acc: 23.958%\t iter-Loss: 1.377\n",
            " train\tAvg. Acc: 26.982%\t Avg. Loss: 0.005\n",
            "Epoch: 001\t Phase: test Iter: 006/006\t iter-Acc: 26.083%\t iter-Loss: 1.383\n",
            " test\tAvg. Acc: 25.118%\t Avg. Loss: 0.001\n",
            "Epoch: 002\t Phase: train Iter: 469/469\t iter-Acc: 26.562%\t iter-Loss: 1.369\n",
            " train\tAvg. Acc: 29.483%\t Avg. Loss: 0.005\n",
            "Epoch: 002\t Phase: test Iter: 006/006\t iter-Acc: 29.667%\t iter-Loss: 1.380\n",
            " test\tAvg. Acc: 29.329%\t Avg. Loss: 0.001\n",
            "Epoch: 003\t Phase: train Iter: 469/469\t iter-Acc: 31.771%\t iter-Loss: 1.357\n",
            " train\tAvg. Acc: 31.831%\t Avg. Loss: 0.005\n",
            "Epoch: 003\t Phase: test Iter: 006/006\t iter-Acc: 33.583%\t iter-Loss: 1.375\n",
            " test\tAvg. Acc: 33.000%\t Avg. Loss: 0.001\n",
            "Epoch: 004\t Phase: train Iter: 469/469\t iter-Acc: 39.583%\t iter-Loss: 1.339\n",
            " train\tAvg. Acc: 34.222%\t Avg. Loss: 0.005\n",
            "Epoch: 004\t Phase: test Iter: 006/006\t iter-Acc: 35.500%\t iter-Loss: 1.367\n",
            " test\tAvg. Acc: 35.013%\t Avg. Loss: 0.001\n",
            "Epoch: 005\t Phase: train Iter: 469/469\t iter-Acc: 40.625%\t iter-Loss: 1.312\n",
            " train\tAvg. Acc: 36.841%\t Avg. Loss: 0.005\n",
            "Epoch: 005\t Phase: test Iter: 006/006\t iter-Acc: 37.667%\t iter-Loss: 1.351\n",
            " test\tAvg. Acc: 37.303%\t Avg. Loss: 0.001\n",
            "Epoch: 006\t Phase: train Iter: 469/469\t iter-Acc: 44.271%\t iter-Loss: 1.269\n",
            " train\tAvg. Acc: 39.628%\t Avg. Loss: 0.005\n",
            "Epoch: 006\t Phase: test Iter: 006/006\t iter-Acc: 40.917%\t iter-Loss: 1.324\n",
            " test\tAvg. Acc: 40.829%\t Avg. Loss: 0.001\n",
            "Epoch: 007\t Phase: train Iter: 469/469\t iter-Acc: 48.438%\t iter-Loss: 1.205\n",
            " train\tAvg. Acc: 43.268%\t Avg. Loss: 0.005\n",
            "Epoch: 007\t Phase: test Iter: 006/006\t iter-Acc: 45.083%\t iter-Loss: 1.279\n",
            " test\tAvg. Acc: 45.303%\t Avg. Loss: 0.001\n",
            "Epoch: 008\t Phase: train Iter: 469/469\t iter-Acc: 53.125%\t iter-Loss: 1.112\n",
            " train\tAvg. Acc: 47.923%\t Avg. Loss: 0.005\n",
            "Epoch: 008\t Phase: test Iter: 006/006\t iter-Acc: 50.250%\t iter-Loss: 1.209\n",
            " test\tAvg. Acc: 50.526%\t Avg. Loss: 0.001\n",
            "Epoch: 009\t Phase: train Iter: 469/469\t iter-Acc: 57.292%\t iter-Loss: 1.004\n",
            " train\tAvg. Acc: 53.368%\t Avg. Loss: 0.004\n",
            "Epoch: 009\t Phase: test Iter: 006/006\t iter-Acc: 55.833%\t iter-Loss: 1.119\n",
            " test\tAvg. Acc: 55.816%\t Avg. Loss: 0.001\n",
            "Epoch: 010\t Phase: train Iter: 469/469\t iter-Acc: 61.979%\t iter-Loss: 0.901\n",
            " train\tAvg. Acc: 58.326%\t Avg. Loss: 0.004\n",
            "Epoch: 010\t Phase: test Iter: 006/006\t iter-Acc: 60.833%\t iter-Loss: 1.028\n",
            " test\tAvg. Acc: 59.855%\t Avg. Loss: 0.001\n",
            "Epoch: 011\t Phase: train Iter: 469/469\t iter-Acc: 65.625%\t iter-Loss: 0.818\n",
            " train\tAvg. Acc: 62.263%\t Avg. Loss: 0.004\n",
            "Epoch: 011\t Phase: test Iter: 006/006\t iter-Acc: 64.500%\t iter-Loss: 0.953\n",
            " test\tAvg. Acc: 63.408%\t Avg. Loss: 0.001\n",
            "Epoch: 012\t Phase: train Iter: 469/469\t iter-Acc: 70.833%\t iter-Loss: 0.740\n",
            " train\tAvg. Acc: 66.037%\t Avg. Loss: 0.003\n",
            "Epoch: 012\t Phase: test Iter: 006/006\t iter-Acc: 66.583%\t iter-Loss: 0.884\n",
            " test\tAvg. Acc: 66.250%\t Avg. Loss: 0.001\n",
            "Epoch: 013\t Phase: train Iter: 469/469\t iter-Acc: 73.438%\t iter-Loss: 0.674\n",
            " train\tAvg. Acc: 69.068%\t Avg. Loss: 0.003\n",
            "Epoch: 013\t Phase: test Iter: 006/006\t iter-Acc: 69.333%\t iter-Loss: 0.824\n",
            " test\tAvg. Acc: 69.250%\t Avg. Loss: 0.001\n",
            "Epoch: 014\t Phase: train Iter: 469/469\t iter-Acc: 75.000%\t iter-Loss: 0.622\n",
            " train\tAvg. Acc: 71.575%\t Avg. Loss: 0.003\n",
            "Epoch: 014\t Phase: test Iter: 006/006\t iter-Acc: 71.333%\t iter-Loss: 0.770\n",
            " test\tAvg. Acc: 72.066%\t Avg. Loss: 0.001\n",
            "Epoch: 015\t Phase: train Iter: 469/469\t iter-Acc: 77.083%\t iter-Loss: 0.585\n",
            " train\tAvg. Acc: 73.690%\t Avg. Loss: 0.003\n",
            "Epoch: 015\t Phase: test Iter: 006/006\t iter-Acc: 73.083%\t iter-Loss: 0.724\n",
            " test\tAvg. Acc: 73.579%\t Avg. Loss: 0.001\n",
            "Epoch: 016\t Phase: train Iter: 469/469\t iter-Acc: 78.125%\t iter-Loss: 0.559\n",
            " train\tAvg. Acc: 75.435%\t Avg. Loss: 0.003\n",
            "Epoch: 016\t Phase: test Iter: 006/006\t iter-Acc: 74.167%\t iter-Loss: 0.684\n",
            " test\tAvg. Acc: 75.013%\t Avg. Loss: 0.001\n",
            "Epoch: 017\t Phase: train Iter: 469/469\t iter-Acc: 79.688%\t iter-Loss: 0.543\n",
            " train\tAvg. Acc: 76.525%\t Avg. Loss: 0.003\n",
            "Epoch: 017\t Phase: test Iter: 006/006\t iter-Acc: 76.250%\t iter-Loss: 0.651\n",
            " test\tAvg. Acc: 76.421%\t Avg. Loss: 0.001\n",
            "Epoch: 018\t Phase: train Iter: 469/469\t iter-Acc: 80.729%\t iter-Loss: 0.527\n",
            " train\tAvg. Acc: 78.055%\t Avg. Loss: 0.002\n",
            "Epoch: 018\t Phase: test Iter: 006/006\t iter-Acc: 76.750%\t iter-Loss: 0.619\n",
            " test\tAvg. Acc: 77.526%\t Avg. Loss: 0.001\n",
            "Epoch: 019\t Phase: train Iter: 469/469\t iter-Acc: 81.771%\t iter-Loss: 0.515\n",
            " train\tAvg. Acc: 79.168%\t Avg. Loss: 0.002\n",
            "Epoch: 019\t Phase: test Iter: 006/006\t iter-Acc: 77.667%\t iter-Loss: 0.593\n",
            " test\tAvg. Acc: 78.539%\t Avg. Loss: 0.000\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def generate_batch(batch):\n",
        "  label = torch.tensor([entry[0]-1 for entry in batch])\n",
        "  texts = [tokenizer(entry[1]) for entry in batch]\n",
        "  offsets = [0] + [len(text) for text in texts]\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  big_text = torch.cat([torch.tensor([vocab[t] if t in stoi else 0 for t in text]) for text in texts])\n",
        "  #big_text = torch.cat([torch.tensor([vocab.stoi[t] for t in text]) for text in texts])\n",
        "\n",
        "  return big_text, offsets, label\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 20\n",
        "TEST_BATCH_SIZE = BATCH_SIZE * 5\n",
        "LR = 1e-1\n",
        "\n",
        "model = CNNClassifier(len(vocab), num_classes=num_classes).to(device)\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "split_size = {'train': len(train_list), 'test': len(test_list)}\n",
        "\n",
        "# train_dataset, test_dataset = AG_NEWS(root=\"data\")\n",
        "for epoch in range(1, NUM_EPOCHS):\n",
        "  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  test_loader = DataLoader(test_list, batch_size=TEST_BATCH_SIZE, collate_fn=generate_batch)\n",
        "  loaders = {'train': train_loader, 'test': test_loader}\n",
        "  for phase in ['train', 'test']:\n",
        "    if phase == 'train':\n",
        "      model.train()\n",
        "    else:\n",
        "      model.eval()\n",
        "\n",
        "    total_acc, total_loss = 0, 0\n",
        "    for i, (texts, offsets, cls) in enumerate(loaders[phase]):\n",
        "      texts = texts.to(device)\n",
        "      offsets = offsets.to(device)\n",
        "      cls = cls.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "        output = model(texts, offsets)\n",
        "        loss = criterion(output, cls)\n",
        "        total_loss += loss.item()\n",
        "        if phase == 'train':\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      acc = (output.argmax(1) == cls).sum().item()\n",
        "      total_acc += acc\n",
        "\n",
        "      sys.stdout.write('\\rEpoch: {0:03d}\\t Phase: {1} Iter: {2:03d}/{3:03d}\\t iter-Acc: {4:.3f}%\\t iter-Loss: {5:.3f}'.format(epoch, phase, i+1, len(loaders[phase]), acc/len(offsets)*100, loss.item()))\n",
        "\n",
        "    if phase == 'train':\n",
        "      scheduler.step()\n",
        "    print('\\n {0}\\tAvg. Acc: {1:.3f}%\\t Avg. Loss: {2:.3f}'.format(phase, total_acc/split_size[phase]*100, total_loss/split_size[phase]))"
      ]
    }
  ]
}